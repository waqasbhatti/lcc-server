# How lcserver-v3 works

## Required input information

Required for everything:

- lcdir
- object catalog generated by `lcproc.make_lclist` with kdtree and all of the
  interesting keys in each light curve's objectinfo (including objectinfo,
  comments, varinfo keys). this should be after the
  `lcproc.add_checkplot_info_lclist` function has been run, so we have those
  star features (color, coord, neighbor, GAIA features) already at hand to use
  in the eventual search space
- checkplots of objects in lcdir (even if they have no period-finder results)

Optional:

- FITS of the field for LCs in lcdir

This allows us to:

- search the objects by objectid, coordinates, and xmatch, and
  objectinfo/varinfo/comment keys
- generate dataset pickles for each search result

The overall directory structure for a light curve collection will look something
like:

basedir/

lcc-index.sqlite -> contains:
                    - name and description of the LC set
                    - footprints for each LC set
                    - basedir paths for each LC set to get to its catalog
                      sqlite, kdtree, and datasets
                    - sets of columns, indexcols and ftscols for all LC sets

lcc-datasets.sqlite -> contains:
                       - dataset locations, nobjects, columns requested etc.


- datasets/-> dataset-{setUUID}.pkl.gz
- products/ -> lightcurves-{setUUID}.zip, dataset-{setUUID}.zip

collection-dir/
- lclist-catalog.pkl -> generated by the `lcproc.make_lclist` function and
  processed through the `lcproc.add_checkplot_info_to_lclist` function

- catalog-kdtree.pkl -> broken out from the lclist-catalog.pkl file

- catalog-objectinfo.sqlite -> broken out from the lclist-catalog.pkl file

generated using something similar to:

lccserver.objectsearch.abcat.objectinfo_to_sqlite(
    'lclist-catalog.pkl',
    'catalog-objectinfo.sqlite',
    lcset_name='<blah>',
    lcset_desc='<blah>',
    lcset_project='<blah>',
    lcset_datarelease=0,
    lcset_citation='<blah>',
)

- lightcurves/ -> whatever subdir structure here is used. all generated CSVLCs
                  are kept in the same directories as the original LCs, but
                  symlinked to here

- periodfinding/ -> periodfinding result pkls from astrobase

- checkplots/    -> checkplot pickles from astrobase


## Dataset pickles

- generated after search completes
  highlighting objects in the search result
- have keys that correspond to:
  - LC location
  - LC information including all objectinfo, comments, varinfo keys
  - checkplot location
- dataset pickles and associated products like the zipfiles, etc. are written to
  a basedir as specified, this could be the actual lcdir (or a subdir)

- this allows us to:

  - browse a table of objects in the search results
  - download selected/all light curves in the search results in original, CSV,
    or CSV gz form bundled up in a zip file
  - download the entire dataset as a zip file generated asynchronously,
    containing the LCs in original form, their checkplots, and a CSV file
    summarizing all of the object information

  - FUTURE: can be set to private, can add comments to the dataset as a whole
  - FUTURE: if FITS is provided, generates overlayed and zoom-contained finder
  - FUTURE: see an overview of each object using tiles generated from the checkplots
    (async load each CP, grab finder, unphased LC, best phased LC if CP is
    finalized, phased LCs using each PF method and best period if CP is not
    finalized
  - FUTURE: allow quick comments for each object on this screen, saving onblur
  - FUTURE: allow opening selected object in a checkplotserver view to edit them
    in more detail
  - FUTURE: download checkplots for selected/all objects


## Serving the data

- tornado search and retrieval server
- endpoints:
  - / -> overview of all of the light curve information, show search bar and
    list of generated public datasets by other people
  - /datasets/<dataset id> -> dataset view endpoint, where <dataset id> is a
    SHA256 generated ID corresponding to <dataset id>.pkl

- can run many tornado servers for a single light curve set, they should be all
  independent and mostly readonly


## later TODO

- add a TAP server using the sqlite and kdtree. This will use sqlparse to parse
  the DML and generate the correct numpy/sqlite statements for selecting things,
  while keeping in mind that no writing to the database is allowed. maybe break
  this out into a separate astrotap-server project

- document search state machine
