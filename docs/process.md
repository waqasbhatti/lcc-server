# How lcserver-v3 works

## Required input information

Required for everything:

- lcdir
- object catalog generated by `lcproc.make_lclist` with kdtree and all of the
  interesting keys in each light curve's objectinfo (including objectinfo,
  comments, varinfo keys). this should be after the
  `lcproc.add_checkplot_info_lclist` function has been run, so we have those
  star features (color, coord, neighbor, GAIA features) already at hand to use
  in the eventual search space
- checkplots of objects in lcdir (even if they have no period-finder results)

Optional:

- FITS of the field for LCs in lcdir

This allows us to:

- search the objects by objectid, coordinates, and xmatch, and
  objectinfo/varinfo/comment keys
- generate dataset pickles for each search result

The overall directory structure for a light curve set will look something like:

basedir/

lcc-collections.sqlite -> contains:
                       - name and description of the LC set
                       - footprints for each LC set
                         (D3 equatorial plot possible for all footprints)
                       - objectids for all LC sets
                       - basedir paths for each LC set to get to its catalog
                         sqlite, kdtree, and datasets
                       - columns, indexcols, fitscols for each dataset
                       - sets of columns, indexcols and ftscols for all LC sets

- datasets/{1st 2 chars in setUUID}/{2nd 2 chars in setUUID}/ ->
  {setUUID}.pkl. this directory is read by an async worker to figure out
  the current active datasets

- products/{1st 2 chars in setUUID}/{2nd 2 chars in setUUID}/
  -> lightcurves-{setUUID}.zip, dataset-{setUUID}.zip


collection-dir/
- lclist-catalog.pkl -> generated by the `lcproc.make_lclist` function and
  processed through the `lcproc.add_checkplot_info_to_lclist` function

- catalog-kdtree.pkl -> broken out from the lclist-catalog.pkl file

- catalog-objectinfo.sqlite -> broken out from the lclist-catalog.pkl file

generated using something similar to:

abcat.objectinfo_to_sqlite(
    'lclist-catalog.pkl',
    'catalog-objectinfo.sqlite',
    lcset_name='HATNet Kepler field',
    lcset_desc='HATNet observations of the Kepler Field from 2004 to 2014',
    lcset_project='HATNet',
    lcset_datarelease=0,
    lcset_citation='Bakos+ 2004',
)

- lightcurves/ -> whatever subdir structure here is used

- periodfinding/

- checkplots/


## Dataset pickles

- generated after search completes
- can be set to private, can add comments to the dataset as a whole
- if FITS is provided, generates overlayed and zoom-contained finder
  highlighting objects in the search result
- have keys that correspond to:
  - LC location
  - LC information including all objectinfo, comments, varinfo keys
  - checkplot location
- this allows us to:
  - browse a table of objects in the search results
  - download selected/all light curves in the search results in original, CSV,
    or CSV gz form bundled up in a zip file
  - see an overview of each object using tiles generated from the checkplots
    (async load each CP, grab finder, unphased LC, best phased LC if CP is
    finalized, phased LCs using each PF method and best period if CP is not
    finalized
    - allow quick comments for each object on this screen, saving onblur
    - allow opening selected object in a checkplotserver view to edit them in
      more detail
  - download checkplots for selected/all objects
  - download the entire dataset as a zip file generated asynchronously,
    containing the LCs in original form, their checkplots, and a CSV file
    summarizing all of the object information

- dataset pickles and associated products like the zipfiles, etc. are written to
  a basedir as specified, this could be the actual lcdir (or a subdir)

## Serving the data

- tornado search and retrieval server
- endpoints:
  - / -> overview of all of the light curve information, show search bar and
    list of generated public datasets by other people
  - /datasets/<dataset id> -> dataset view endpoint, where <dataset id> is a
    SHA256 generated ID corresponding to <dataset id>.pkl

- can run many tornado servers for a single light curve set, they should be all
  independent and mostly readonly


## later TODO

- add a TAP server using the sqlite and kdtree. This will use sqlparse to parse
  the DML and generate the correct numpy/sqlite statements for selecting things,
  while keeping in mind that no writing to the database is allowed. maybe break
  this out into a separate astrotap-server project



## Implement the following

- dataset pickle creation with SHA256 sum, listing, deletion, editing

- light curve zip creation with SHA256 sum, deletion, editing when dataset
  edited

- checkplot zip creation with SHA256 sum, deletion, editing when dataset edited

- periodfinding zip creation with SHA256 sum, deletion, editing when dataset
  edited

- frontend hookup. Use ProcessPoolExecutor.
  - search functions: FTS, coord, xmatch, column search, parse coord strings
  - dataset functions: list all datasets, show dataset
  -

- JS hookup, simple POST processing only for now.
  - later, using async JSON load for dataset processing.
